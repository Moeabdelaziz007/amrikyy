#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Feedback Weighting System for AuraOS Learning Growth
Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© ÙˆØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
"""

import asyncio
import json
import time
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, deque
from dataclasses import dataclass
from enum import Enum
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FeedbackType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"
    CORRECTION = "correction"
    ENHANCEMENT = "enhancement"

class FeedbackSource(Enum):
    """Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
    USER_RATING = "user_rating"
    USER_COMMENT = "user_comment"
    SYSTEM_METRICS = "system_metrics"
    PERFORMANCE_DATA = "performance_data"
    IMPLICIT_BEHAVIOR = "implicit_behavior"

@dataclass
class FeedbackEntry:
    """Ø¥Ø¯Ø®Ø§Ù„ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©"""
    id: str
    pattern_id: str
    feedback_type: FeedbackType
    source: FeedbackSource
    score: float  # -1.0 Ø¥Ù„Ù‰ 1.0
    weight: float  # ÙˆØ²Ù† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
    timestamp: datetime
    context: Dict[str, Any]
    metadata: Dict[str, Any]

@dataclass
class PatternPerformance:
    """Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø·"""
    pattern_id: str
    total_feedback: int
    positive_feedback: int
    negative_feedback: int
    weighted_score: float
    confidence_adjustment: float
    last_updated: datetime

class AdvancedFeedbackWeighting:
    """Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
    
    def __init__(self):
        self.feedback_history: Dict[str, List[FeedbackEntry]] = defaultdict(list)
        self.pattern_performance: Dict[str, PatternPerformance] = {}
        self.feedback_weights: Dict[FeedbackSource, float] = {
            FeedbackSource.USER_RATING: 1.0,
            FeedbackSource.USER_COMMENT: 0.8,
            FeedbackSource.SYSTEM_METRICS: 0.6,
            FeedbackSource.PERFORMANCE_DATA: 0.7,
            FeedbackSource.IMPLICIT_BEHAVIOR: 0.4
        }
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        self.config = {
            "min_feedback_count": 3,
            "confidence_threshold": 0.7,
            "weight_decay_factor": 0.95,
            "learning_rate": 0.1,
            "feedback_window_hours": 24,
            "adaptive_weighting": True
        }
        
        # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        self.sentiment_analyzer = SentimentAnalyzer()
        self.weight_calculator = WeightCalculator()
        self.performance_tracker = PerformanceTracker()
        
        logger.info("ğŸ¯ Advanced Feedback Weighting System initialized")
    
    async def process_feedback(self, pattern_id: str, feedback_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        try:
            # 1. ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
            feedback_type = await self._analyze_feedback_type(feedback_data)
            
            # 2. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØµØ¯Ø±
            source = await self._identify_feedback_source(feedback_data)
            
            # 3. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆØ§Ù„ÙˆØ²Ù†
            score = await self._calculate_feedback_score(feedback_data, feedback_type)
            weight = await self._calculate_feedback_weight(feedback_data, source)
            
            # 4. Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
            feedback_entry = FeedbackEntry(
                id=f"feedback_{int(time.time())}_{hash(str(feedback_data))[:8]}",
                pattern_id=pattern_id,
                feedback_type=feedback_type,
                source=source,
                score=score,
                weight=weight,
                timestamp=datetime.now(),
                context=feedback_data.get("context", {}),
                metadata=feedback_data.get("metadata", {})
            )
            
            # 5. Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ®
            self.feedback_history[pattern_id].append(feedback_entry)
            
            # 6. ØªØ­Ø¯ÙŠØ« Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø·
            await self._update_pattern_performance(pattern_id, feedback_entry)
            
            # 7. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ
            await self._apply_adaptive_learning(pattern_id)
            
            return {
                "feedback_id": feedback_entry.id,
                "pattern_id": pattern_id,
                "feedback_type": feedback_type.value,
                "source": source.value,
                "score": score,
                "weight": weight,
                "pattern_performance": self.pattern_performance.get(pattern_id).__dict__ if pattern_id in self.pattern_performance else None,
                "timestamp": feedback_entry.timestamp.isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ Error processing feedback: {e}")
            return {"error": str(e)}
    
    async def _analyze_feedback_type(self, feedback_data: Dict[str, Any]) -> FeedbackType:
        """ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ù‹Ø§
        if "text" in feedback_data:
            sentiment = await self.sentiment_analyzer.analyze(feedback_data["text"])
            
            if sentiment > 0.6:
                return FeedbackType.POSITIVE
            elif sentiment < 0.4:
                return FeedbackType.NEGATIVE
            else:
                return FeedbackType.NEUTRAL
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠ
        if "rating" in feedback_data:
            rating = feedback_data["rating"]
            if rating >= 4:
                return FeedbackType.POSITIVE
            elif rating <= 2:
                return FeedbackType.NEGATIVE
            else:
                return FeedbackType.NEUTRAL
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª
        if "action" in feedback_data:
            action = feedback_data["action"].lower()
            if "correct" in action or "fix" in action:
                return FeedbackType.CORRECTION
            elif "improve" in action or "enhance" in action:
                return FeedbackType.ENHANCEMENT
            elif "good" in action or "great" in action:
                return FeedbackType.POSITIVE
            elif "bad" in action or "wrong" in action:
                return FeedbackType.NEGATIVE
        
        return FeedbackType.NEUTRAL
    
    async def _identify_feedback_source(self, feedback_data: Dict[str, Any]) -> FeedbackSource:
        """ØªØ­Ø¯ÙŠØ¯ Ù…ØµØ¯Ø± Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        if "user_rating" in feedback_data:
            return FeedbackSource.USER_RATING
        elif "user_comment" in feedback_data or "text" in feedback_data:
            return FeedbackSource.USER_COMMENT
        elif "system_metrics" in feedback_data:
            return FeedbackSource.SYSTEM_METRICS
        elif "performance_data" in feedback_data:
            return FeedbackSource.PERFORMANCE_DATA
        elif "implicit_behavior" in feedback_data:
            return FeedbackSource.IMPLICIT_BEHAVIOR
        else:
            return FeedbackSource.USER_COMMENT  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
    
    async def _calculate_feedback_score(self, feedback_data: Dict[str, Any], feedback_type: FeedbackType) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        base_score = 0.0
        
        # Ù†Ù‚Ø§Ø· Ø£Ø³Ø§Ø³ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        if feedback_type == FeedbackType.POSITIVE:
            base_score = 0.8
        elif feedback_type == FeedbackType.NEGATIVE:
            base_score = -0.8
        elif feedback_type == FeedbackType.CORRECTION:
            base_score = -0.6
        elif feedback_type == FeedbackType.ENHANCEMENT:
            base_score = 0.6
        else:  # NEUTRAL
            base_score = 0.0
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠ
        if "rating" in feedback_data:
            rating = feedback_data["rating"]
            # ØªØ­ÙˆÙŠÙ„ Ù…Ù† 1-5 Ø¥Ù„Ù‰ -1 Ø¥Ù„Ù‰ 1
            normalized_rating = (rating - 3) / 2
            base_score = normalized_rating
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©
        confidence = feedback_data.get("confidence", 1.0)
        base_score *= confidence
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù‚Ø§Ø· ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        return max(-1.0, min(1.0, base_score))
    
    async def _calculate_feedback_weight(self, feedback_data: Dict[str, Any], source: FeedbackSource) -> float:
        """Ø­Ø³Ø§Ø¨ ÙˆØ²Ù† Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
        base_weight = self.feedback_weights[source]
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙˆØ²Ù† Ø­Ø³Ø¨ Ø¹ÙˆØ§Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠØ©
        weight_multiplier = 1.0
        
        # Ø¹Ø§Ù…Ù„ Ø§Ù„ÙˆÙ‚Øª (Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„Ù‡Ø§ ÙˆØ²Ù† Ø£ÙƒØ¨Ø±)
        if "timestamp" in feedback_data:
            feedback_time = datetime.fromisoformat(feedback_data["timestamp"])
            time_diff = datetime.now() - feedback_time
            if time_diff < timedelta(hours=1):
                weight_multiplier *= 1.2
            elif time_diff > timedelta(days=7):
                weight_multiplier *= 0.8
        
        # Ø¹Ø§Ù…Ù„ Ø§Ù„ØªÙØµÙŠÙ„ (Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ÙØµÙ„Ø© Ù„Ù‡Ø§ ÙˆØ²Ù† Ø£ÙƒØ¨Ø±)
        if "text" in feedback_data and len(feedback_data["text"]) > 50:
            weight_multiplier *= 1.1
        
        # Ø¹Ø§Ù…Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ù„Ù‡Ø§ ÙˆØ²Ù† Ø£ÙƒØ¨Ø±)
        if "context" in feedback_data and feedback_data["context"]:
            weight_multiplier *= 1.05
        
        return base_weight * weight_multiplier
    
    async def _update_pattern_performance(self, pattern_id: str, feedback_entry: FeedbackEntry):
        """ØªØ­Ø¯ÙŠØ« Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø·"""
        if pattern_id not in self.pattern_performance:
            self.pattern_performance[pattern_id] = PatternPerformance(
                pattern_id=pattern_id,
                total_feedback=0,
                positive_feedback=0,
                negative_feedback=0,
                weighted_score=0.0,
                confidence_adjustment=0.0,
                last_updated=datetime.now()
            )
        
        performance = self.pattern_performance[pattern_id]
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª
        performance.total_feedback += 1
        
        if feedback_entry.score > 0:
            performance.positive_feedback += 1
        elif feedback_entry.score < 0:
            performance.negative_feedback += 1
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙˆØ²ÙˆÙ†Ø©
        total_weighted_score = performance.weighted_score * (performance.total_feedback - 1)
        total_weighted_score += feedback_entry.score * feedback_entry.weight
        performance.weighted_score = total_weighted_score / performance.total_feedback
        
        # Ø­Ø³Ø§Ø¨ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©
        if performance.total_feedback >= self.config["min_feedback_count"]:
            success_rate = performance.positive_feedback / performance.total_feedback
            
            if success_rate >= 0.8:
                performance.confidence_adjustment = 0.1
            elif success_rate >= 0.6:
                performance.confidence_adjustment = 0.05
            elif success_rate <= 0.4:
                performance.confidence_adjustment = -0.1
            else:
                performance.confidence_adjustment = 0.0
        
        performance.last_updated = datetime.now()
        
        logger.info(f"ğŸ“Š Updated performance for pattern {pattern_id}: "
                   f"score={performance.weighted_score:.3f}, "
                   f"confidence_adj={performance.confidence_adjustment:.3f}")
    
    async def _apply_adaptive_learning(self, pattern_id: str):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙƒÙŠÙÙŠ"""
        if pattern_id not in self.pattern_performance:
            return
        
        performance = self.pattern_performance[pattern_id]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¯Ù‡ÙˆØ± Ø§Ù„Ø²Ù…Ù†ÙŠ
        time_since_update = datetime.now() - performance.last_updated
        if time_since_update > timedelta(hours=self.config["feedback_window_hours"]):
            performance.confidence_adjustment *= self.config["weight_decay_factor"]
        
        # ØªØ­Ø¯ÙŠØ« Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if self.config["adaptive_weighting"]:
            await self._update_source_weights(pattern_id)
    
    async def _update_source_weights(self, pattern_id: str):
        """ØªØ­Ø¯ÙŠØ« Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if pattern_id not in self.feedback_history:
            return
        
        recent_feedback = [
            f for f in self.feedback_history[pattern_id]
            if datetime.now() - f.timestamp < timedelta(hours=24)
        ]
        
        if len(recent_feedback) < 5:
            return
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© ÙƒÙ„ Ù…ØµØ¯Ø±
        source_accuracy = defaultdict(list)
        
        for feedback in recent_feedback:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø¯Ù‚Ø© Ø§Ù„Ù…ØµØ¯Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§ØªØ³Ø§Ù‚
            accuracy = abs(feedback.score) * feedback.weight
            source_accuracy[feedback.source].append(accuracy)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        for source, accuracies in source_accuracy.items():
            if len(accuracies) >= 2:
                avg_accuracy = np.mean(accuracies)
                # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙˆØ²Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø©
                adjustment = (avg_accuracy - 0.5) * 0.1
                self.feedback_weights[source] = max(0.1, min(2.0, 
                    self.feedback_weights[source] + adjustment))
    
    async def get_pattern_recommendations(self, pattern_id: str) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù†Ù…Ø·"""
        if pattern_id not in self.pattern_performance:
            return {"error": "Pattern not found"}
        
        performance = self.pattern_performance[pattern_id]
        
        recommendations = {
            "pattern_id": pattern_id,
            "current_performance": {
                "weighted_score": performance.weighted_score,
                "confidence_adjustment": performance.confidence_adjustment,
                "total_feedback": performance.total_feedback,
                "success_rate": performance.positive_feedback / max(1, performance.total_feedback)
            },
            "recommendations": []
        }
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if performance.weighted_score < -0.3:
            recommendations["recommendations"].append({
                "type": "improvement_needed",
                "priority": "high",
                "message": "Pattern needs significant improvement",
                "suggested_actions": ["review_pattern_logic", "gather_more_feedback", "adjust_confidence"]
            })
        elif performance.weighted_score > 0.7:
            recommendations["recommendations"].append({
                "type": "excellent_performance",
                "priority": "low",
                "message": "Pattern performing excellently",
                "suggested_actions": ["maintain_current_approach", "consider_pattern_replication"]
            })
        
        if performance.total_feedback < self.config["min_feedback_count"]:
            recommendations["recommendations"].append({
                "type": "insufficient_data",
                "priority": "medium",
                "message": "Need more feedback for reliable assessment",
                "suggested_actions": ["encourage_user_feedback", "monitor_implicit_signals"]
            })
        
        return recommendations
    
    async def get_system_analytics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        total_patterns = len(self.pattern_performance)
        total_feedback = sum(p.total_feedback for p in self.pattern_performance.values())
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡
        avg_performance = np.mean([p.weighted_score for p in self.pattern_performance.values()]) if total_patterns > 0 else 0
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±
        source_distribution = defaultdict(int)
        for feedback_list in self.feedback_history.values():
            for feedback in feedback_list:
                source_distribution[feedback.source] += 1
        
        return {
            "total_patterns": total_patterns,
            "total_feedback": total_feedback,
            "average_performance": avg_performance,
            "source_distribution": dict(source_distribution),
            "feedback_weights": {k.value: v for k, v in self.feedback_weights.items()},
            "high_performing_patterns": len([p for p in self.pattern_performance.values() if p.weighted_score > 0.7]),
            "low_performing_patterns": len([p for p in self.pattern_performance.values() if p.weighted_score < -0.3]),
            "timestamp": datetime.now().isoformat()
        }

class SentimentAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
    
    async def analyze(self, text: str) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù†Øµ"""
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        positive_words = ["good", "great", "excellent", "amazing", "wonderful", "perfect", "love", "like"]
        negative_words = ["bad", "terrible", "awful", "horrible", "worst", "hate", "dislike", "wrong"]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return 0.8
        elif negative_count > positive_count:
            return 0.2
        else:
            return 0.5

class WeightCalculator:
    """Ø­Ø§Ø³Ø¨Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
    
    def __init__(self):
        self.base_weights = {
            FeedbackSource.USER_RATING: 1.0,
            FeedbackSource.USER_COMMENT: 0.8,
            FeedbackSource.SYSTEM_METRICS: 0.6,
            FeedbackSource.PERFORMANCE_DATA: 0.7,
            FeedbackSource.IMPLICIT_BEHAVIOR: 0.4
        }

class PerformanceTracker:
    """Ù…ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    
    def __init__(self):
        self.performance_history = deque(maxlen=1000)
    
    async def track_performance(self, pattern_id: str, performance_data: Dict[str, Any]):
        """ØªØªØ¨Ø¹ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø·"""
        self.performance_history.append({
            "pattern_id": pattern_id,
            "timestamp": datetime.now(),
            "data": performance_data
        })

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
async def demo_feedback_weighting():
    """Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù†Ø¸Ø§Ù… ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
    print("ğŸ¯ Advanced Feedback Weighting Demo")
    print("=" * 50)
    
    weighting_system = AdvancedFeedbackWeighting()
    
    # Ø¨ÙŠØ§Ù†Ø§Øª ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    test_feedback = [
        {
            "pattern_id": "pattern_001",
            "rating": 5,
            "text": "Great response, very helpful!",
            "context": {"user_level": "beginner"}
        },
        {
            "pattern_id": "pattern_001",
            "rating": 2,
            "text": "Not what I was looking for",
            "context": {"user_level": "advanced"}
        },
        {
            "pattern_id": "pattern_002",
            "rating": 4,
            "text": "Good but could be better",
            "context": {"user_level": "intermediate"}
        },
        {
            "pattern_id": "pattern_001",
            "rating": 5,
            "text": "Perfect! Exactly what I needed",
            "context": {"user_level": "beginner"}
        }
    ]
    
    for i, feedback_data in enumerate(test_feedback, 1):
        print(f"\nğŸ“ Processing Feedback {i}:")
        print(f"   Pattern: {feedback_data['pattern_id']}")
        print(f"   Rating: {feedback_data['rating']}")
        print(f"   Text: {feedback_data['text']}")
        
        result = await weighting_system.process_feedback(
            feedback_data["pattern_id"], 
            feedback_data
        )
        
        if "error" not in result:
            print(f"   Feedback Type: {result['feedback_type']}")
            print(f"   Source: {result['source']}")
            print(f"   Score: {result['score']:.3f}")
            print(f"   Weight: {result['weight']:.3f}")
        
        await asyncio.sleep(0.1)
    
    # Ø¹Ø±Ø¶ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print(f"\nğŸ“Š System Analytics:")
    analytics = await weighting_system.get_system_analytics()
    
    for key, value in analytics.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")
    
    # Ø¹Ø±Ø¶ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø·
    print(f"\nğŸ¯ Pattern Recommendations:")
    for pattern_id in ["pattern_001", "pattern_002"]:
        recommendations = await weighting_system.get_pattern_recommendations(pattern_id)
        if "error" not in recommendations:
            print(f"\n   Pattern {pattern_id}:")
            print(f"     Performance Score: {recommendations['current_performance']['weighted_score']:.3f}")
            print(f"     Success Rate: {recommendations['current_performance']['success_rate']:.3f}")
            print(f"     Total Feedback: {recommendations['current_performance']['total_feedback']}")

if __name__ == "__main__":
    asyncio.run(demo_feedback_weighting())
