# AuraOS Learning Loop Training - Google Colab Setup
# Ø¥Ø¹Ø¯Ø§Ø¯ Google Colab Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø­Ù„Ù‚Ø§Øª

"""
ğŸš€ Ø¥Ø¹Ø¯Ø§Ø¯ Google Colab Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø­Ù„Ù‚Ø§Øª

Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯ Ø´Ø§Ù…Ù„ Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ AuraOS
Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Colab Ù…Ø¹ Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°ÙƒÙŠ.

Ø§Ù„Ù…ÙŠØ²Ø§Øª:
- ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ BERT/Transformer
- Ø­Ù„Ù‚Ø© ØªØ¹Ù„Ù… Ø°ÙƒÙŠØ© Ù…Ø¹ Early Stopping
- Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø¹ Weights & Biases
- ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†ØªØ§Ø¦Ø¬
- ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø¹ GPU
"""

# =============================================================================
# ğŸ“‹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
# =============================================================================

def install_requirements():
    """ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    import subprocess
    import sys
    
    packages = [
        "torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
        "transformers datasets accelerate",
        "wandb tensorboard",
        "scikit-learn pandas numpy matplotlib seaborn",
        "jupyter ipywidgets"
    ]
    
    for package in packages:
        subprocess.check_call([sys.executable, "-m", "pip", "install"] + package.split())

# =============================================================================
# ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©
# =============================================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

def setup_environment():
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Weights & Biases Ù„Ù„ØªØ¹Ù‚Ø¨
    try:
        import wandb
        wandb.init(project="auraos-learning-loop", entity="your-username")
        print("âœ… Weights & Biases initialized")
    except:
        print("âš ï¸ Weights & Biases not available")
    
    return device

# =============================================================================
# ğŸ“Š ÙØ¦Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ©
# =============================================================================

class AuraOSDataset(Dataset):
    """
    Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø®ØµØµØ© Ù„Ù€ AuraOS Learning Loop
    
    Ø§Ù„Ù…ÙŠØ²Ø§Øª:
    - Ø¯Ø¹Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    - ØªØ±Ù…ÙŠØ² ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ø¹ BERT
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ
        encoding = self.tokenizer(
            item['text'],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(item['label'], dtype=torch.long)
        }

# =============================================================================
# ğŸ§  Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
# =============================================================================

class AuraOSLearningModel(nn.Module):
    """
    Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù€ AuraOS
    
    Ø§Ù„Ù…ÙŠØ²Ø§Øª:
    - Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ BERT/Transformer
    - Ø·Ø¨Ù‚Ø§Øª ØªØµÙ†ÙŠÙ Ù…Ø®ØµØµØ©
    - Ø¯Ø¹Ù… Dropout Ù„Ù„ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† Overfitting
    """
    
    def __init__(self, model_name='bert-base-uncased', num_classes=10, dropout=0.3):
        super(AuraOSLearningModel, self).__init__()
        
        # Ù†Ù…ÙˆØ°Ø¬ BERT Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        from transformers import AutoModel
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
        
    def forward(self, input_ids, attention_mask):
        # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø± BERT
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… [CLS] token Ù„Ù„ØªØµÙ†ÙŠÙ
        pooled_output = outputs.pooler_output
        output = self.dropout(pooled_output)
        return self.classifier(output)

# =============================================================================
# ğŸ”„ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°ÙƒÙŠØ©
# =============================================================================

class SmartLearningLoop:
    """
    Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°ÙƒÙŠØ© Ù„Ù€ AuraOS
    
    Ø§Ù„Ù…ÙŠØ²Ø§Øª:
    - Early Stopping
    - Learning Rate Scheduling
    - Gradient Clipping
    - Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
    - Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    """
    
    def __init__(self, model, train_loader, val_loader, device):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # Ù…Ø­Ø³Ù† Ù…Ø¹ ØªØ¹Ù„Ù… Ù…ØªÙƒÙŠÙ
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=2e-5,
            weight_decay=0.01
        )
        
        # Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=3,
            verbose=True
        )
        
        # Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        self.criterion = nn.CrossEntropyLoss()
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []
        
    def train_epoch(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø­Ù‚Ø¨Ø© ÙˆØ§Ø­Ø¯Ø©"""
        self.model.train()
        total_loss = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¬Ù‡Ø§Ø²
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
            self.optimizer.zero_grad()
            
            # Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ
            outputs = self.model(input_ids, attention_mask)
            loss = self.criterion(outputs, labels)
            
            # Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø®Ù„ÙÙŠ
            loss.backward()
            
            # Ù‚Øµ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ù„Ù…Ù†Ø¹ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Weights & Biases
            try:
                import wandb
                wandb.log({"batch_loss": loss.item()})
            except:
                pass
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø¯Ù…
            if batch_idx % 100 == 0:
                print(f"Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}")
        
        return total_loss / len(self.train_loader)
    
    def validate(self):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        avg_loss = total_loss / len(self.val_loader)
        
        return avg_loss, accuracy
    
    def train(self, epochs=10, early_stopping_patience=5):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        best_val_loss = float('inf')
        patience_counter = 0
        
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        print("=" * 60)
        
        for epoch in range(epochs):
            print(f"\nğŸ“Š Epoch {epoch+1}/{epochs}")
            print("-" * 50)
            
            # ØªØ¯Ø±ÙŠØ¨
            train_loss = self.train_epoch()
            
            # ØªØ­Ù‚Ù‚
            val_loss, val_accuracy = self.validate()
            
            # ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            self.scheduler.step(val_loss)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_accuracy)
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            print(f"ğŸ“ˆ Train Loss: {train_loss:.4f}")
            print(f"ğŸ“‰ Val Loss: {val_loss:.4f}")
            print(f"ğŸ¯ Val Accuracy: {val_accuracy:.2f}%")
            print(f"ğŸ“š Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Weights & Biases
            try:
                import wandb
                wandb.log({
                    "epoch": epoch + 1,
                    "train_loss": train_loss,
                    "val_loss": val_loss,
                    "val_accuracy": val_accuracy,
                    "learning_rate": self.optimizer.param_groups[0]['lr']
                })
            except:
                pass
            
            # Early Stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
                torch.save(self.model.state_dict(), 'best_model.pth')
                print("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬")
            else:
                patience_counter += 1
                print(f"â³ Early Stopping Counter: {patience_counter}/{early_stopping_patience}")
                
            if patience_counter >= early_stopping_patience:
                print(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break
        
        print("\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
        return self.train_losses, self.val_losses, self.val_accuracies
    
    def plot_training_history(self):
        """Ø±Ø³Ù… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Ø±Ø³Ù… Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        ax1.plot(self.train_losses, label='Train Loss', color='blue', linewidth=2)
        ax1.plot(self.val_losses, label='Validation Loss', color='red', linewidth=2)
        ax1.set_title('ğŸ“Š Training and Validation Loss', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Ø±Ø³Ù… Ø§Ù„Ø¯Ù‚Ø©
        ax2.plot(self.val_accuracies, label='Validation Accuracy', color='green', linewidth=2)
        ax2.set_title('ğŸ¯ Validation Accuracy', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# =============================================================================
# ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
# =============================================================================

def analyze_performance(model, test_loader, device):
    """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø¯Ø§Ø¡"""
    model.eval()
    all_predictions = []
    all_labels = []
    
    print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids, attention_mask)
            _, predicted = torch.max(outputs, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            if batch_idx % 50 == 0:
                print(f"Processed {batch_idx}/{len(test_loader)} batches")
    
    # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
    print("\nğŸ“‹ Classification Report:")
    print("=" * 50)
    print(classification_report(all_labels, all_predictions))
    
    # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(all_labels, all_predictions)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=range(len(set(all_labels))),
                yticklabels=range(len(set(all_labels))))
    plt.title('ğŸ” Confusion Matrix', fontsize=16, fontweight='bold')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
    
    return all_predictions, all_labels

# =============================================================================
# ğŸš€ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# =============================================================================

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    print("ğŸš€ AuraOS Learning Loop Training")
    print("=" * 50)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©
    device = setup_environment()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© (Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©)
    sample_data = [
        {'text': 'This is a positive example', 'label': 1},
        {'text': 'This is a negative example', 'label': 0},
        # Ø£Ø¶Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ù†Ø§
    ]
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_data, val_data = train_test_split(sample_data, test_size=0.2, random_state=42)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_dataset = AuraOSDataset(train_data, tokenizer)
    val_dataset = AuraOSDataset(val_data, tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = AuraOSLearningModel(num_classes=2)  # ØªØµÙ†ÙŠÙ Ø«Ù†Ø§Ø¦ÙŠ
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù…
    learning_loop = SmartLearningLoop(model, train_loader, val_loader, device)
    
    # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    train_losses, val_losses, val_accuracies = learning_loop.train(epochs=10)
    
    # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    learning_loop.plot_training_history()
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
    predictions, labels = analyze_performance(model, val_loader, device)
    
    print("\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
    
    return model, learning_loop

# =============================================================================
# ğŸ”§ ÙˆØ¸Ø§Ø¦Ù Ù…Ø³Ø§Ø¹Ø¯Ø©
# =============================================================================

def monitor_gpu():
    """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU"""
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
        print(f"Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
    else:
        print("GPU ØºÙŠØ± Ù…ØªØ§Ø­")

def save_model(model, path='auraos_model.pth'):
    """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    torch.save(model.state_dict(), path)
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {path}")

def load_model(model, path='auraos_model.pth'):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    model.load_state_dict(torch.load(path))
    print(f"ğŸ“‚ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {path}")
    return model

# =============================================================================
# ğŸ¯ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# =============================================================================

if __name__ == "__main__":
    # ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
    # install_requirements()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    model, learning_loop = main()
    
    # Ù…Ø±Ø§Ù‚Ø¨Ø© GPU
    monitor_gpu()
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    save_model(model)
    
    print("\nğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")
